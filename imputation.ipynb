{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "This notebook contains two sections. First, in pre-processing, the raw data that was extracted from 3DStock, is organised into a form compatible with SimStock and assigns usages, wwr values etc. \n",
    "\n",
    "In the second section, missing values are then imputed.\n",
    "\n",
    ":warning: Ensure you are using SimStock v0.2.0 \n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "This section contains the preprocessing steps that get the data into the correct format for Simstock. First thing we need to do is sort out floor numbering. The data contains a row for each floor. The floors are numbered in ascending order but there are a couple of cases that need to be handled. Mezzanine levels had been (by some convention) given floor number -7 and lower ground floors are given -0.5. I propose we change -0.5 to -1 so that lower ground floors are just a basement. And I propose we simply record mezzanines as a first floor and shuffle all floors above up by 1. There are only 3 instances of -0.5 and 20 or so of -7, so the results shouldn't be very sensitive to these choices anyway. There is also one -99 that detones \"a range of floors\". This is a mistake in the data, so I suggest we delete that row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv(\"data/simstock_v2_pt2_selection_202203170624.csv\")\n",
    "\n",
    "# Delete the row where \"floor\" is equal to -99\n",
    "df = df[df[\"floor\"] != -99]\n",
    "\n",
    "# Get unique scu_id values that have at least one mezzanine level (-7)\n",
    "buildings_with_mezzanines = df[df[\"floor\"] == -7][\"scu_id\"].unique()\n",
    "\n",
    "# Define a function to transform the floors for buildings with mezzanines\n",
    "def transform_floors_for_building(building_df):\n",
    "\n",
    "    # Check this building does indeed have a mezzanine\n",
    "    has_mezzanine = -7 in building_df[\"floor\"].values\n",
    "    if has_mezzanine:\n",
    "\n",
    "        # Shuffle all upstairs floors up by one to make room\n",
    "        building_df.loc[building_df[\"floor\"] > 0, \"floor\"] += 1\n",
    "\n",
    "        # Set mezzanine to floor 1\n",
    "        building_df.loc[building_df[\"floor\"] == -7, \"floor\"] = 1\n",
    "\n",
    "    return building_df\n",
    "\n",
    "\n",
    "# Apply the transformation to each building with mezzanines\n",
    "for scu_id in buildings_with_mezzanines:\n",
    "    transformed_building = transform_floors_for_building(\n",
    "            df[df[\"scu_id\"] == scu_id]\n",
    "        )\n",
    "    df.loc[df[\"scu_id\"] == scu_id] = transformed_building\n",
    "\n",
    "# Now we need to do the same with the -0.5 floors. Set them to -1.\n",
    "# If there is also a basement, then shuffle down by 1 so it becomes\n",
    "# a lower basement\n",
    "# Get unique scu_id values that have at least one mezzanine level (-7)\n",
    "buildings_with_lower = df[df[\"floor\"] == -0.5][\"scu_id\"].unique()\n",
    "\n",
    "# Define a function to transform the floors for buildings with lower floors\n",
    "def transform_floors_for_building_with_lower(building_df):\n",
    "\n",
    "    # Check this building does indeed have a mezzanine\n",
    "    has_lower = -0.5 in building_df[\"floor\"].values\n",
    "    if has_lower:\n",
    "\n",
    "        # Shuffle all basement floors down by one to make room\n",
    "        building_df.loc[building_df[\"floor\"] <= -1, \"floor\"] -= 1\n",
    "\n",
    "        # Set mezzanine to floor 1\n",
    "        building_df.loc[building_df[\"floor\"] == -0.5, \"floor\"] = -1\n",
    "\n",
    "    return building_df\n",
    "\n",
    "# Apply the transformation to each building with lower floors\n",
    "for scu_id in buildings_with_lower:\n",
    "    transformed_building = transform_floors_for_building_with_lower(\n",
    "            df[df[\"scu_id\"] == scu_id]\n",
    "        )\n",
    "    df.loc[df[\"scu_id\"] == scu_id] = transformed_building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now all of the floors in df should be numbered sensibly. Now let's create another dataframe where each row corresponds to a single scu and the columns contain data for Simstock. This will be the dataframe we put into Simstock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.wkt import loads\n",
    "\n",
    "# Unique buildings\n",
    "unique_scu_ids = df[\"scu_id\"].unique()\n",
    "\n",
    "# Create a new dataframe, where each row is a unique SCU\n",
    "new_df = pd.DataFrame(index=unique_scu_ids, columns=[\"nofloors\", \"polygon\", \"shading\", \"height\", \"wwr\", \"construction\", \"age\"])\n",
    "\n",
    "# Populate the nofloors, geometry and shading columns\n",
    "for scu_id in unique_scu_ids:\n",
    "\n",
    "    # Select all the rows from the df that have a given scu_id\n",
    "    rows_for_scu_id = df[df[\"scu_id\"] == scu_id]\n",
    "\n",
    "    # Calculate the count of occurrences for the current scu_id\n",
    "    count = len(rows_for_scu_id)\n",
    "\n",
    "    # Populate the \"nofloors\" column with the count\n",
    "    new_df.loc[scu_id, \"nofloors\"] = count\n",
    "    \n",
    "    # Populate the \"polygon\" column with the first instance's \"wkb_geometry\" value\n",
    "    new_df.loc[scu_id, \"polygon\"] = loads(rows_for_scu_id.iloc[0][\"wkb_geometry\"])\n",
    "\n",
    "    # Set everything to be a non-shading object\n",
    "    new_df.loc[scu_id, \"shading\"] = False\n",
    "\n",
    "    new_df.loc[scu_id, \"age\"] = rows_for_scu_id.iloc[0][\"age_ukbuildings_overall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also now have height data. Lets add this in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in height data\n",
    "height_df = pd.read_csv(\"data/_select_b_scu_id_a_mean_object_height_cm_0_01_as_mean_hgt_m_a_es_202310241411.csv\")\n",
    "\n",
    "# Create a dictionary to store the mean height values for each 'scu_id'\n",
    "mean_height_dict = height_df.groupby('scu_id')['mean_hgt_m'].mean().to_dict()\n",
    "\n",
    "# Add height data to our main dataframe\n",
    "new_df['height'] = new_df.index.map(mean_height_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the data that there are around 145 SCU entries that are actually multi-polygons. They are not trivial multi-polygons as they contain more than one polygon (SimStock can handle trivial multi-polygons). All instances of these non-trivial multi-polygons are for purely domestic buildings, and all seem to be where there is one main building with one or more smaller buildings. Let's plot a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can see from the data that the following SCUs are actually multi-polygons\n",
    "scu1 = 52401019330040\n",
    "scu2 = 52401020930003\n",
    "\n",
    "# Lets plot them\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "object = new_df.loc[scu1, 'polygon']\n",
    "for polyon in list(object.geoms):\n",
    "     x, y = polyon.exterior.xy\n",
    "     axes[0].plot(x, y)\n",
    "object = new_df.loc[scu2, 'polygon']\n",
    "for polyon in list(object.geoms):\n",
    "     x, y = polyon.exterior.xy\n",
    "     axes[1].plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are houses with a shed(s) or garage, maybe some kind of granny annex. So lets just discard the sheds and keep only the main house. Clearly, we'll be losing cases where the shed/garage is powered or houses some kind of home hobbyist activity etc. \n",
    "\n",
    "For now, let's define a discard sheds function. This could potentially be incorporated into Simstock later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def discard_sheds(object: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to take a wkt representation of a \n",
    "    shapely object and, if it is a non-trivial\n",
    "    MultiPolygon, discard all but the largest\n",
    "    of the Polygons.\n",
    "    \"\"\"\n",
    "    poly = object\n",
    "\n",
    "    # If it is an ordinary polygon, \n",
    "    # then do nothing to it\n",
    "    if isinstance(poly, Polygon):\n",
    "        return poly\n",
    "\n",
    "    # Now lets find the polygon \n",
    "    # with the largest area and \n",
    "    # discard the rest\n",
    "    largest_polygon = None\n",
    "    largest_area = 0.0\n",
    "\n",
    "    # Iterate through all polygons in the MultiPolygon\n",
    "    for polygon in list(poly.geoms):\n",
    "        area = polygon.area\n",
    "        if area > largest_area:\n",
    "            largest_polygon = polygon\n",
    "            largest_area = area\n",
    "\n",
    "    if largest_polygon is not None:\n",
    "        # Return the largest polygon as a separate Polygon object\n",
    "        return largest_polygon\n",
    "    else:\n",
    "        # If no valid polygon is found, return None\n",
    "        return None\n",
    "    \n",
    "# Now lets apply this function to the dataframe\n",
    "new_df['polygon'] = new_df['polygon'].map(discard_sheds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new dataframe where each row is a unique SCU. \"nofloors\", \"polygon\", \"height\", \"age\", and \"shading\" columns have been populated. We still need to populate \"wwr\", and \"construction\". We also need to add usage columns for each floor. Let's do this next, by add in 20 floor usage fields for each row. Of course not every row will have anywhere near 20 floors, so most of the entries will end up being blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of floors\n",
    "max_floors = new_df.nofloors.max()\n",
    "\n",
    "# Add columns named \"FLOOR_1: use\", \"FLOOR_2: use\", etc.\n",
    "# Add 20 new columns with empty values\n",
    "for i in range(1, max_floors+1):\n",
    "    new_df[f\"FLOOR_{i}: use\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to match the carb3 activities in the csv file to simstock usages. Here are the carb3 activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carb3_actvities = df[\"ndtype_1_carb3_activity\"].unique()\n",
    "print(carb3_actvities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the uses available in the Simstock database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights = pd.read_csv(\"data/settings/DB-Loads-LIGHTS.csv\")\n",
    "print(lights.Name.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a dictionary that maps the one to the other. Any case that doesn't have an activity will default to \"Dwell\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should all sanity check this\n",
    "use_map = {\n",
    "    \"Workshop\": \"Factory\",\n",
    "    \"Shop NEC\": \"Shop\",\n",
    "    \"Store\": \"Shop\", # Assuming this is American for shop? Or is it a warehouse?\n",
    "    \"Office (Inc Computer Centres) NEC\": \"Office\",\n",
    "    \"Village hall, Scout hut, Guide hut\": \"Community\",\n",
    "    \"State school\": \"Education\",\n",
    "    \"Office (HQ / Institutional)\": \"Office\",\n",
    "    \"Public House/Pub Restaurant\": \"Hospitality\",\n",
    "    \"Restaurant\": \"Hospitality\",\n",
    "    \"Wine bar\": \"Hospitality\",\n",
    "    \"Car Showroom / Sales Site\": \"Shop\",\n",
    "    \"Cafe\": \"Hospitality\",\n",
    "    \"Warehouse\": \"Warehouse\",\n",
    "    \"Bank/ insurance/ building society branch\": \"Shop\",\n",
    "    \"Leisure centre (with swimming)\": \"Sport\",\n",
    "    \"Multi-storey car parks\": \"Transport\",\n",
    "    \"Hairdressing/Beauty Salon\": \"Shop\", # Might be more similar to health?\n",
    "    \"Fire station\": \"Emergency\",\n",
    "    \"Vehicle repair workshop\": \"Factory\", # Or transport?\n",
    "    \"Gymnasium, fitness centre\": \"Sport\",\n",
    "    \"Private School / College\": \"Education\",\n",
    "    \"Surgery / Clinic / Health Centre\": \"Health\",\n",
    "    \"Nursery, Creche, Playschool, Childcare\": \"Education\",\n",
    "    \"Municipal Occupation NEC\": \"Education\", # not sure what this is\n",
    "    \"Nightclub, discotheque\": \"Hospitality\",\n",
    "    \"Sports Ground\": \"Sport\"\n",
    "}\n",
    "\n",
    "# Now wrap the map in a function\n",
    "def usage_mapper(use: str) -> str:\n",
    "    try:\n",
    "        return use_map[use]\n",
    "    except KeyError:\n",
    "        # When there is no listed usage, assume it is Dwell\n",
    "        return \"Dwell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go through the SCUs and look up each or their row's activity. Run this activity through the map and use it to fill in the values of \"FLOOR_1: use\", \"FLOOR_2: use\", etc. in the new dataframe. We will also look to see if the building has any glazing percentage epc data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over unique scu_ids in original dataframe\n",
    "for scu_id in unique_scu_ids:\n",
    "\n",
    "    # Filter rows for the current \"scu_id\"\n",
    "    current_building = df[df[\"scu_id\"] == scu_id]\n",
    "\n",
    "    # Add this glazing data to the new_df\n",
    "    new_df.at[scu_id, \"wwr\"] = current_building[\"d_epc_glazing_pct\"].mean()\n",
    "\n",
    "    # Iterate over the rows (floors) of the building\n",
    "    # in ascending order of floor number\n",
    "    current_building = current_building.sort_values(by=\"floor\")\n",
    "    i = 1\n",
    "    for _, row in current_building.iterrows():\n",
    "\n",
    "        # Access this floor's ndtype_1_carb3_activity\n",
    "        use = row[\"ndtype_1_carb3_activity\"]\n",
    "\n",
    "        # Map it to the simplified uses\n",
    "        activity = usage_mapper(use)\n",
    "        \n",
    "        # Put this into the corresponding floor usage column \n",
    "        # in the new df, at the row corresponding to \n",
    "        # this scu_id\n",
    "        new_df.at[scu_id, f\"FLOOR_{i}: use\"] = activity\n",
    "\n",
    "        # Increment floor counter\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "This section uses machine learning methods to impute missing values. This section also performs cross-validation of these imputation methods.\n",
    "\n",
    "First, what percentage of buildings have glazing percentage, age and height data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_with_wwr = (new_df[\"wwr\"].count() / len(new_df)) * 100\n",
    "percentage_with_age = (new_df[\"age\"].count() / len(new_df)) * 100\n",
    "percentage_with_height = (new_df[\"height\"].count() / len(new_df)) * 100\n",
    "mean_wwr = new_df.wwr.mean()\n",
    "print(f\"Percentage with wwr: {percentage_with_wwr}\")\n",
    "print(f\"Mean wwr: {mean_wwr}\")\n",
    "print(f\"Percentage with age: {percentage_with_age}\")\n",
    "print(f\"Unique ages: {new_df.age.unique()}\")\n",
    "print(f\"Percentage with height: {percentage_with_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about half have glazing percentage. And there are a few instances of missing height and age data. Let's impute these missing values. Lets start by imputing the age and height data. Height data is obviously continuous, so a good option is K-nearst-neighbours (KNN) imputation. Age data is catagorical, so we need a classification algorithm; let's use a support vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Extract building centroids and add them to the DataFrame as new columns\n",
    "# This will help us use location for imputation\n",
    "new_df[\"centroid_x\"] = new_df[\"polygon\"].apply(lambda geom: geom.centroid.x)\n",
    "new_df[\"centroid_y\"] = new_df[\"polygon\"].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "# Let's first impute height based on location \n",
    "# (there is only one missing height value) so\n",
    "# we don't need to worry too much about methods here\n",
    "# Train the KNN model\n",
    "df_with_height = new_df.dropna(subset=['height'])\n",
    "df_without_height = new_df[new_df['height'].isnull()] # This is only one value\n",
    "regression_model = KNeighborsRegressor(n_neighbors=20, weights=\"distance\")\n",
    "regression_model.fit(df_with_height[[\"centroid_x\", \"centroid_y\"]], df_with_height[\"height\"])\n",
    "\n",
    "# # Use the trained model to predict the missing \"height\" value\n",
    "predicted_height = regression_model.predict(df_without_height[[\"centroid_x\", \"centroid_y\"]])\n",
    "print(predicted_height)\n",
    "df_without_height = df_without_height.assign(height = predicted_height)\n",
    "new_df = pd.concat([df_with_height, df_without_height])\n",
    "\n",
    "percentage_with_height = (new_df[\"height\"].count() / len(new_df)) * 100\n",
    "print(f\"Percentage with height: {percentage_with_height}\")\n",
    "\n",
    "# We will now use impute the 9% or so of age categories\n",
    "# that are missing. First, let's validate the imputing method. \n",
    "# We first need to encode the string-based age data numerically\n",
    "age_mapping = {\"PRE-1914\": 1, \"1918-1939\": 2, \"1945-1980\": 3, \"POST-1980\": 4}\n",
    "new_df[\"encoded_age\"] = new_df[\"age\"].map(age_mapping)\n",
    "\n",
    "# Also create an inverse map so we can recover the string values\n",
    "inv_age_mapping = {v: k for k, v in age_mapping.items()}\n",
    "\n",
    "# Now split the data into the chunk that has age and the chunk that doesnt\n",
    "df_with_age = new_df.dropna(subset=['encoded_age'])\n",
    "df_without_age = new_df[new_df['encoded_age'].isnull()]\n",
    "\n",
    "# Create a support vector classifier\n",
    "classifier = SVC()\n",
    "\n",
    "# Define a scoring function to calculate mean absolute percentage error (MAPE)\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# Perform 5-fold cross-validation for the support vector machine\n",
    "# Note that the negative sign below is just sign convention for\n",
    "# scikit learn's implementation of 5-fold cv\n",
    "features = [\"centroid_x\", \"centroid_y\", \"height\"]\n",
    "target = \"encoded_age\"\n",
    "classifier_scores = -cross_val_score(classifier, df_with_age[features], df_with_age[target], cv=5, scoring=mape_scorer)\n",
    "\n",
    "# Calculate the average and standard deviation of MSE scores\n",
    "average_mape = np.mean(classifier_scores)\n",
    "\n",
    "print(\"SVM Age Classifier Performance in 5-fold Cross-Validation:\")\n",
    "print(\"Average MAPE:\", average_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work quite well so let's proceed and use the SVM to impute the missing ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the svm\n",
    "classifier.fit(df_with_age[features], df_with_age[target])\n",
    "\n",
    "# Use the trained model to predict missing age values\n",
    "predicted_ages = classifier.predict(df_without_age[features])\n",
    "df_without_age = df_without_age.assign(encoded_age = predicted_ages)\n",
    "new_df = pd.concat([df_with_age, df_without_age])\n",
    "\n",
    "# And recover the string values\n",
    "new_df[\"age\"] = new_df[\"encoded_age\"].map(inv_age_mapping)\n",
    "\n",
    "# Now check we have all age and height data\n",
    "percentage_with_age = (new_df[\"age\"].count() / len(new_df)) * 100\n",
    "percentage_with_height = (new_df[\"height\"].count() / len(new_df)) * 100\n",
    "print(f\"Percentage with age: {percentage_with_age}\")\n",
    "print(f\"Unique ages: {new_df.age.unique()}\")\n",
    "print(f\"Percentage with height: {percentage_with_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to proceed to imputing the missing wwr values. There are a lot of these to impute: nearly half the data is missing. We can use height, age and location as predictors of wwr. We have many algorithms to choose from to do this. Let's choose linear regression and k-nearest-neighbours. Let's cross-validate them and see which performs the best. We will do each for both with and without location data, to see if location is a useful predictor of wwr. This is what we will do here:\n",
    "- Validation of linear regression (both with and without location data)\n",
    "- Validation of KNN for a range of n-neighbour values (both with and without location data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Start by splitting the DataFrame into two parts: \n",
    "# one with missing \"wwr\" and one without\n",
    "df_without_wwr = new_df[new_df[\"wwr\"].isna()]\n",
    "df_with_wwr = new_df[~new_df[\"wwr\"].isna()]\n",
    "\n",
    "# Perform 5-fold cross-validation for the linear regression\n",
    "# without the location data\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "features = [\"encoded_age\", \"height\"]\n",
    "target = \"wwr\"\n",
    "lr_scores = -cross_val_score(LinearRegression(), df_with_wwr[features], df_with_wwr[target], cv=5, scoring=mape_scorer)\n",
    "average_mape_lr_noloc = np.mean(lr_scores)\n",
    "\n",
    "# Perform 5-fold cross-validation for the linear regression\n",
    "# with the location data\n",
    "features = [\"centroid_x\", \"centroid_y\", \"encoded_age\", \"height\"]\n",
    "target = \"wwr\"\n",
    "lr_scores = -cross_val_score(LinearRegression(), df_with_wwr[features], df_with_wwr[target], cv=5, scoring=mape_scorer)\n",
    "average_mape_lr_withloc = np.mean(lr_scores)\n",
    "\n",
    "# Now lets cross-validate KNN for a range of n-neighbour values\n",
    "# Define a parameter grid with different values of n_neighbors to search\n",
    "param_grid = {'n_neighbors': [i for i in range(1,50)]}\n",
    "\n",
    "# Need to adjust the scorer for the way KNN works\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=True)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "# This will iterate over all values of the n_neighbour\n",
    "# parameter and perform cross_validation using 5-fold \n",
    "# splitting on each. The splitting is only applied to \n",
    "# the section of the data that already has wwr values\n",
    "grid_search = GridSearchCV(estimator=KNeighborsRegressor(weights='distance'), param_grid=param_grid, scoring=mape_scorer, cv=5)\n",
    "features = [\"encoded_age\", \"height\"]\n",
    "target = \"wwr\"\n",
    "grid_search.fit(df_with_wwr[features], df_with_wwr[target]) \n",
    "\n",
    "# Get the validation scores (they come as a df)\n",
    "knn_noloc_cv_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Repeat above, this time including location data\n",
    "grid_search = GridSearchCV(estimator=KNeighborsRegressor(weights='distance'), param_grid=param_grid, scoring=mape_scorer, cv=5)\n",
    "features = [\"centroid_x\", \"centroid_y\", \"encoded_age\", \"height\"]\n",
    "target = \"wwr\"\n",
    "grid_search.fit(df_with_wwr[features], df_with_wwr[target])\n",
    "\n",
    "# Get the validation scores (save as a df)\n",
    "knn_withloc_cv_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Print results\n",
    "lowest_kknmape_noloc = min(knn_noloc_cv_df.mean_test_score.values)\n",
    "lowest_kknmape_withloc = min(knn_withloc_cv_df.mean_test_score.values)\n",
    "print(f\"MAPE for linear regression without location data: {average_mape_lr_noloc}\")\n",
    "print(f\"MAPE for linear regression with location data: {average_mape_lr_withloc}\")\n",
    "print(f\"Lowest MAPE for knn without location data: {lowest_kknmape_noloc}\")\n",
    "print(f\"Lowest MAPE for knn with location data: {lowest_kknmape_withloc}\")\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(5, 4))\n",
    "axes.set_title(\"5-fold cross-validation of K-nearest neighbours\")\n",
    "axes.plot(knn_noloc_cv_df.param_n_neighbors.values, knn_noloc_cv_df.mean_test_score.values, label=\"KNN, no location data\")\n",
    "axes.plot(knn_withloc_cv_df.param_n_neighbors.values, knn_withloc_cv_df.mean_test_score.values, label=\"KNN, with location data\")\n",
    "axes.axhline(average_mape_lr_noloc, color=\"k\", linestyle=\"--\", label=\"Linear regression, no location data\")\n",
    "axes.legend(loc=1)\n",
    "axes.set_xlabel(\"Nearest neighbours, $n$\")\n",
    "axes.set_ylabel(\"MAPE\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the average MAPE scores for KNN for increasing values of the nearest neighbours parameter $n$. At each value of $n$, the model was tested using 5-fold cross-validation. The black line represents the MAPE of linear regression with no location data. It is worth noting that linear regression with location data performed very badly. The best performer here seems to be KNN without location data with a nearest neighbours parameter of more than about 15. Both linear regression and KNN perform worse when they make use of location data. I think I might know why this is. Most of the missing WWR values are in purely non-domestic buildings. Looking at them on a map, most of these premises are in areas of the high street or in commercial estates where there is a great deal of heterogeneity between adjacent buildings. I think it is understandable that proximity is not a good predictor in these cases. For domestic buildings, such as in housing estates, I can believe that proximity would be a good predictor. However, we have the WWR data for most of these domestic buildings anyway.\n",
    "\n",
    "We could go further and use more complicated methods. E.g. we could partition the data into geographical regions or clusters, and then have different regression models in each. Or we could come up with detailed heuristics and decision processes to assign WWR values. I don't think we should do this though. Such methods would be very hard to validate. And given the relatively small size of the data set and the large number of missing values, I don't think it is a fruitful line of work given the difficulty of it and the limited if any improvements it would offer.\n",
    "\n",
    "So let's go ahead and impute the missing WWR values using KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the KNN model\n",
    "regression_model = KNeighborsRegressor(n_neighbors=20, weights=\"distance\")\n",
    "regression_model.fit(df_with_wwr[[\"height\", \"encoded_age\"]], df_with_wwr[\"wwr\"])\n",
    "\n",
    "# Use the trained model to predict missing \"wwr\" values\n",
    "predicted_wwr = regression_model.predict(df_without_wwr[[\"height\", \"encoded_age\"]])\n",
    "df_without_wwr = df_without_wwr.assign(wwr = predicted_wwr)\n",
    "new_df = pd.concat([df_with_wwr, df_without_wwr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that we've imputed all missing WWR values and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check we have all age and height data\n",
    "percentage_with_wwr = (new_df[\"wwr\"].count() / len(new_df)) * 100\n",
    "print(f\"Percentage with wwr: {percentage_with_wwr}\")\n",
    "\n",
    "plt.hist(df_with_wwr.wwr.values, alpha=0.6, bins=30, label=\"Existing wwr values\")\n",
    "plt.hist(df_without_wwr.wwr.values, alpha=0.8, bins=30, label=\"Imputed wwr values\")\n",
    "plt.xlabel(\"WWR\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've imputed all missing WWR values with (from the cross-validation step earlier) an accuracy rate of around 36%. We can see above that the imputation has roughly captured the skewed distribution of the existing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to assign construction types. There are a lot of different ways we could go about this. I propose the following method. We use all the available data we have for a row, such as any epc entries, ages, usage types etc. to create clusters of different types of buildings. For each cluster we create a construction type. In the limit where we choose a cluster size of 1, then we will be assigning individual bespoke constructions to each and every building. On the other extreme, if we choose a very large cluster size then we will end up with only a handful of clusters that will serve as construction archetypes. We want to be somewhere in the middle in our approach. A small number of clusters will lead to a more archetypey approach, but will be simpler, whereas lots of clusters might be more realistic but very time consuming to assign constructions. We want to use as few clusters as we can without sacrificing too much complexity. \n",
    "\n",
    "To strike the right balance of clustering, let's use the \"elbow\" technique to test K-means clustering. This works by clustering the data for decreasing cluster sizes whilst recordering some metric(s) of in-cluster variance. As the cluster size decreases, we should observe some point of diminishing returns. This point will indicate a maximal cluster size that captures the complexity of the data.\n",
    "\n",
    "Before we do this, let's split the data into purely non-domestic and everything else. This is because the purely non-domestic data has very different data entries (i.e. no EPC entries). Therefore it is best to treat this sperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine the type for each SCU\n",
    "def determine_type(row):\n",
    "    floorcols = row.filter(like=\"FLOOR_\", axis=0)\n",
    "    if \"Dwell\" not in floorcols.values:\n",
    "        return \"nondom\"\n",
    "    elif any((col != \"\" and col != \"Dwell\") for col in floorcols):\n",
    "        return \"mixed\"\n",
    "    else:\n",
    "        return \"dom\"\n",
    "\n",
    "# Record type in a new column\n",
    "new_df[\"type\"] = new_df.apply(determine_type, axis=1)\n",
    "\n",
    "# Split the DataFrame into two based on the \"type\" column\n",
    "nondom_df = new_df[new_df[\"type\"] == \"nondom\"]\n",
    "dom_df = new_df[new_df[\"type\"] != \"nondom\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's save this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"data/unclustered_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cluster the domestic and mixed data. We can use all the EPC data we have, as well as height, age and location. This is a mix of categorical and numerical data. Therefore let's use the so-called k-prototypes clustering method. This is an extension to K-means that can handle a range of data type using suitable encodings. First, we'll need to pull in any useful EPC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_epc_val(rows: pd.DataFrame, field_name: str) -> str:\n",
    "    for i in range(len(rows)):\n",
    "        strval = rows.iloc[i][field_name]\n",
    "        if pd.notna(strval):\n",
    "            return strval\n",
    "    return pd.NA\n",
    "\n",
    "# Pull in EPC data\n",
    "epc_fields = [\n",
    "    \"epc_rating_current\",\n",
    "    \"epc_rating_potential\",\n",
    "    \"d_epc_mainht_fuel\",\n",
    "    \"d_epc_mainht_plant\",\n",
    "    \"d_epc_mainht_room\",\n",
    "    \"d_epc_secondht_fuel\",\n",
    "    \"d_epc_dhw_fuel\",\n",
    "    \"d_epc_dhw_plant\",\n",
    "    \"d_epc_envelope_flr_type\",\n",
    "    \"d_epc_envelope_flr_insulation\",\n",
    "    \"d_epc_envelope_rf_type\",\n",
    "    \"d_epc_envelope_rf_insulation\",\n",
    "    \"d_epc_envelope_wall_type\",\n",
    "    \"d_epc_envelope_wall_insulation\",\n",
    "    \"d_epc_glazing_type\"\n",
    "]\n",
    "# for field in epc_fields:\n",
    "#     dom_df[field] = \"pd.NA\"\n",
    "df_without_wwr = df_without_wwr.assign(wwr = predicted_wwr)\n",
    "for scu in dom_df.index:\n",
    "    rows = df[df[\"scu_id\"] == scu]\n",
    "    for field in epc_fields:\n",
    "        dom_df.at[scu, field] = get_epc_val(rows, field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make sure the EPC values have sensible missing value formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update missing values in the specified columns\n",
    "dom_df.to_csv(\"data/domtest.csv\")\n",
    "nondom_df.to_csv(\"data/nondomtest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few options for checking the clustering methods. I propose we split the domestic data into missing epc and existing epc. We then want to test epc imputation methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with at least one pd.NA in the epc_fields columns\n",
    "na_rows = dom_df[epc_fields].apply(lambda row: any(pd.isna(value) for value in row), axis=1)\n",
    "\n",
    "# Split the DataFrame\n",
    "dom_df_without_epc = dom_df[na_rows]\n",
    "dom_df_with_epc = dom_df[~na_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many have complete EPC data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dom_df_without_epc)/len(dom_df_with_epc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 60% of the domestic data has full EPC values. Now let's cluster the data with epc values using K-prototypes for increasing cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# I need a function that takes my dataframe and a range of cluster numbers\n",
    "# and returns, for each cluster number, the scores of that clustering\n",
    "def compute_cluster_curve(\n",
    "        df: pd.DataFrame, \n",
    "        fields: list,\n",
    "        cluster_nums: list,\n",
    "        measure: object = None,\n",
    "        verbose: bool = True\n",
    "        ) -> tuple:\n",
    "    \n",
    "    # Subset the DataFrame with the selected columns\n",
    "    subset_df = df[fields]\n",
    "\n",
    "    # Separate numerical and categorical columns\n",
    "    num_cols = subset_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cat_cols = subset_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Standardize numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    subset_df[num_cols] = scaler.fit_transform(subset_df[num_cols])\n",
    "\n",
    "    # Convert categorical columns to string type\n",
    "    subset_df[cat_cols] = subset_df[cat_cols].astype(str)\n",
    "\n",
    "    # Lists to hold the scores\n",
    "    inertias = []\n",
    "    if measure != None:\n",
    "        measures = []\n",
    "\n",
    "    # Iterate over the cluster values\n",
    "    for i in cluster_nums:\n",
    "        if verbose:\n",
    "            print(f\"Clustering with {i} clusters.\")\n",
    "\n",
    "        # Create k-prototypes model\n",
    "        kproto = KPrototypes(n_clusters=i, init='Cao', verbose=0)\n",
    "\n",
    "        # Fit the model\n",
    "        clusters = kproto.fit_predict(subset_df.values, categorical=list(range(len(num_cols), len(subset_df.columns))))\n",
    "\n",
    "        # Record scores\n",
    "        inertias.append(kproto.cost_)\n",
    "        if measure != None:\n",
    "            measures.append(measure(clusters))\n",
    "\n",
    "    if measure != None:\n",
    "        return cluster_nums, inertias, measures\n",
    "    return cluster_nums, inertias\n",
    "\n",
    "\n",
    "# The columns we'd like to use for clustering\n",
    "cluster_col_list = [\"nofloors\", \"height\", \"age\", \"FLOOR_1: use\", *epc_fields]\n",
    "\n",
    "# Range of cluster values to try\n",
    "cluster_nums = np.arange(1,35)\n",
    "\n",
    "print(f\"Number of entries = {len(dom_df_with_epc)}\")\n",
    "\n",
    "res = compute_cluster_curve(dom_df_with_epc,cluster_col_list,cluster_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the res data\n",
    "plt.plot(res[0],res[1],\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the rate of change of the cluster inertias\n",
    "def finite_difference_derivative(arr, step_size=1):\n",
    "    \"\"\"\n",
    "    Compute the derivative of a 1D numpy array using finite differences.\n",
    "    \"\"\"\n",
    "    # Using central difference for interior points\n",
    "    arr = np.array(arr)\n",
    "    derivative = (arr[2:] - arr[:-2])/(2*step_size)\n",
    "\n",
    "    # Using forward/backward difference for boundary points\n",
    "    derivative = np.concatenate(([arr[1] - arr[0]], derivative, [arr[-1] - arr[-2]])) / step_size\n",
    "\n",
    "    return derivative\n",
    "\n",
    "dinertia_dc = finite_difference_derivative(res[1])\n",
    "plt.plot(res[0],dinertia_dc ,\"o\", color=\"m\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Rate of change of inertia\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a clustering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df: pd.DataFrame,\n",
    "            fields: list,\n",
    "            cluster_num: int\n",
    "            ) -> tuple[list, pd.DataFrame]:\n",
    "    \n",
    "    # Subset the DataFrame with the selected columns\n",
    "    subset_df = df[fields]\n",
    "\n",
    "    # Separate numerical and categorical columns\n",
    "    num_cols = subset_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cat_cols = subset_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Standardize numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    subset_df[num_cols] = scaler.fit_transform(subset_df[num_cols])\n",
    "\n",
    "    # Convert categorical columns to string type\n",
    "    subset_df[cat_cols] = subset_df[cat_cols].astype(str)\n",
    "\n",
    "    # Create k-prototypes model\n",
    "    kproto = KPrototypes(n_clusters=cluster_num, init='Cao', verbose=0)\n",
    "\n",
    "    # Fit the model\n",
    "    clusters = kproto.fit_predict(subset_df.values, categorical=list(range(len(num_cols), len(subset_df.columns))))\n",
    "\n",
    "    # Add the cluster labels back to the original DataFrame\n",
    "    dom_df_with_epc['Cluster'] = clusters\n",
    "\n",
    "    # Extract typical values for each cluster\n",
    "    typical_values = pd.DataFrame(columns=cluster_col_list + ['Cluster'])\n",
    "    for cluster_label in dom_df_with_epc['Cluster'].unique():\n",
    "        cluster_data = dom_df_with_epc[dom_df_with_epc['Cluster'] == cluster_label]\n",
    "        typical_values = typical_values.append(cluster_data[cluster_col_list].mode().iloc[0], ignore_index=True)\n",
    "        typical_values.at[typical_values.index[-1], 'Cluster'] = cluster_label\n",
    "    return clusters, typical_values\n",
    "\n",
    "\n",
    "# The columns we'd like to use for clustering\n",
    "cluster_col_list = [\"nofloors\", \"height\", \"age\", \"FLOOR_1: use\", *epc_fields]\n",
    "\n",
    "# Now find typical values for different clusters\n",
    "clusters, typical_values = cluster(dom_df_with_epc, cluster_col_list, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"d_epc_envelope_rf_type\"\n",
    "mismatch_sum = 0\n",
    "for c in set(clusters):\n",
    "\n",
    "    # Get typical value for each cluster\n",
    "    typical_epc = typical_values.iloc[c][target_variable]\n",
    "\n",
    "    # Look up how many rows are in that clsuter\n",
    "    filtered_rows = dom_df_with_epc[dom_df_with_epc['Cluster'] == c]\n",
    "\n",
    "    # Use boolean indexing to filter rows where the column is not equal\n",
    "    # to the typical value\n",
    "    rows_not_equal_to_specified_string = filtered_rows[filtered_rows[target_variable] != typical_epc]\n",
    "\n",
    "    # Count the number of rows\n",
    "    num_rows_not_equal_to_specified_string = len(rows_not_equal_to_specified_string)\n",
    "\n",
    "    mismatch_sum += num_rows_not_equal_to_specified_string\n",
    "\n",
    "print(f\"Mean mismatch of EPC = {mismatch_sum/len(dom_df_with_epc)}\")\n",
    "\n",
    "def percentage_mismatch(row):\n",
    "    cluster_label = row['Cluster']\n",
    "    mismatch_count = 0\n",
    "\n",
    "    for col_name in cluster_col_list:\n",
    "        typical_value = typical_values.iloc[cluster_label][col_name]\n",
    "        if row[col_name] != typical_value:\n",
    "            mismatch_count += 1\n",
    "\n",
    "    total_attributes = len(cluster_col_list)\n",
    "    return (mismatch_count / total_attributes)\n",
    "\n",
    "# Apply the function to each row\n",
    "dom_df_with_epc['MismatchPercentage'] = dom_df_with_epc.apply(percentage_mismatch, axis=1)\n",
    "print(f\"Mean disagreement = {dom_df_with_epc['MismatchPercentage'].mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now redesign the clustering curve function tominclude the Gower distance and the mean disagreement and misassignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intertias, entropies = res[1], res[2]\n",
    "plt.plot(cluster_nums, intertias, \"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We've now filled in all the fields for Simstock and handled MutliPolygon cases. Let's save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"data/processed_croydon.csv\", index_label=\"SCU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simstock-paper-w0-rI819-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
